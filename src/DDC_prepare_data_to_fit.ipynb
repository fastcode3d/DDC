{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoProcessor:\n",
    "    def __init__(self, \n",
    "                 modelFile=\"../models/res10_300x300_ssd_iter_140000.caffemodel\",\n",
    "                 configFile = \"../models/deploy.prototxt\",\n",
    "                 max_interations = 10,\n",
    "                 conf_threshold = 0.60,\n",
    "                 nframesdiff = 5,\n",
    "                 normalized_dim = (d,d)):\n",
    "        self.modelFile = modelFile\n",
    "        self.configFile = configFile\n",
    "        self.max_interations = max_interations\n",
    "        self.conf_threshold = conf_threshold\n",
    "        self.nframesdiff = nframesdiff\n",
    "        self.normalized_dim = normalized_dim\n",
    "        self.net = cv2.dnn.readNetFromCaffe(self.configFile, self.modelFile)\n",
    "        self.mean = 128\n",
    "\n",
    "    def extract_face(self, img):\n",
    "        (h, w) = img.shape[:2]\n",
    "        face = None\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(img, (300, 300)), 1.0, (300, 300), (103.93, 116.77, 123.68))\n",
    "        self.net.setInput(blob)\n",
    "        detections = self.net.forward()\n",
    "        for i in range(detections.shape[2]):\n",
    "            if detections[0, 0, i, 2] > self.conf_threshold:\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (x1, y1, x2, y2) = box.astype(\"int\")\n",
    "                face = img[y1:y2, x1:x2]\n",
    "                face = cv2.resize(face, self.normalized_dim)\n",
    "                break\n",
    "                    \n",
    "        return face\n",
    "\n",
    "    def extract_random_faces(self, filename, num_faces):\n",
    "        captured_faces = []\n",
    "        iterations = 0\n",
    "        v_cap = cv2.VideoCapture(filename)\n",
    "        v_length = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        while len(captured_faces) < num_faces and iterations < self.max_interations:\n",
    "            iterations += 1\n",
    "            v_cap.set(1, np.random.randint(v_length)-1)\n",
    "            \n",
    "            ret, img = v_cap.read()\n",
    "        \n",
    "            if ret == True:\n",
    "                (h, w) = img.shape[:2]\n",
    "                blob = cv2.dnn.blobFromImage(cv2.resize(img, (300, 300)), 1.0, (300, 300), (103.93, 116.77, 123.68))\n",
    "                self.net.setInput(blob)\n",
    "                detections = self.net.forward()\n",
    "                for i in range(detections.shape[2]):\n",
    "                    confidence = detections[0, 0, i, 2]            \n",
    "                    if confidence > self.conf_threshold:\n",
    "                        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                        (x1, y1, x2, y2) = box.astype(\"int\")\n",
    "                        face=img[y1:y2, x1:x2]\n",
    "                        \n",
    "                        # normlize\n",
    "                        face = cv2.resize(face, self.normalized_dim)\n",
    "                        captured_faces.append(face)\n",
    "        \n",
    "        # When everything done, release the video capture and video write objects\n",
    "        v_cap.release()\n",
    "    \n",
    "        return captured_faces\n",
    "  \n",
    "    def extract_random_diff(self, filename, num_diff):\n",
    "        captured_diff = np.empty(shape=(0,d,d,3), dtype=np.int8)\n",
    "        iterations = 0\n",
    "        v_cap = cv2.VideoCapture(filename)\n",
    "        v_length = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        while len(captured_diff) < num_diff and iterations < self.max_interations:\n",
    "            iterations += 1\n",
    "            frame = np.random.randint(v_length)-1\n",
    "            v_cap.set(1, frame)\n",
    "            ret, img_base = v_cap.read()\n",
    "            if ret == True:\n",
    "                v_cap.set(1, frame + self.nframesdiff)\n",
    "                ret, img = v_cap.read()\n",
    "                \n",
    "            if ret == True:\n",
    "                img_base = cv2.cvtColor(img_base, cv2.COLOR_BGR2RGB)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                face1 = self.extract_face(img_base)\n",
    "                face2 = self.extract_face(img)\n",
    "                if type(face1) == type(face2):\n",
    "                    face_diff = cv2.absdiff(face2,face1)\n",
    "                    face_diff = cv2.absdiff(self.mean,face_diff)\n",
    "                    if face_diff is not None:\n",
    "                        captured_diff = np.append(captured_diff,[face_diff],axis=0)\n",
    "        \n",
    "        # When everything done, release the video capture and video write objects\n",
    "        v_cap.release()\n",
    "        \n",
    "        # Closes all the frames\n",
    "        cv2.destroyAllWindows() \n",
    "\n",
    "        return captured_diff\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfdc_train_part_1\n",
      "dfdc_train_part_14\n",
      "dfdc_train_part_48\n",
      "dfdc_train_part_3\n",
      "dfdc_train_part_2\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for root, dirs, files in os.walk('../videos', topdown=False):\n",
    "    for name in dirs:\n",
    "        print(name)\n",
    "        dfdir = pd.read_json('../videos/' + name + '/metadata.json')\n",
    "        dfdir = dfdir.T\n",
    "        dfdir['dir'] = name\n",
    "        df = df.append(dfdir)\n",
    "df[\"processed\"] = 'False'\n",
    "\n",
    "df.to_csv('../data/metadata.csv',index_label='video')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video</th>\n",
       "      <th>label</th>\n",
       "      <th>original</th>\n",
       "      <th>split</th>\n",
       "      <th>dir</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>yakzirnamy.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>jeyvgixnkm.mp4</td>\n",
       "      <td>train</td>\n",
       "      <td>dfdc_train_part_14</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4407</th>\n",
       "      <td>trnznvybjo.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>sudzolvppu.mp4</td>\n",
       "      <td>train</td>\n",
       "      <td>dfdc_train_part_48</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4912</th>\n",
       "      <td>gbinyqapyk.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>ozgcmnllow.mp4</td>\n",
       "      <td>train</td>\n",
       "      <td>dfdc_train_part_48</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6295</th>\n",
       "      <td>yjfdnylpab.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>ndqxtifvbw.mp4</td>\n",
       "      <td>train</td>\n",
       "      <td>dfdc_train_part_48</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8736</th>\n",
       "      <td>aaujbbfhqu.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>qiyzfjrloz.mp4</td>\n",
       "      <td>train</td>\n",
       "      <td>dfdc_train_part_2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4368</th>\n",
       "      <td>hqeldyhmpu.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>vdqritvjfl.mp4</td>\n",
       "      <td>train</td>\n",
       "      <td>dfdc_train_part_48</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9447</th>\n",
       "      <td>rhncgvckxz.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>fanibwbmoq.mp4</td>\n",
       "      <td>train</td>\n",
       "      <td>dfdc_train_part_2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9680</th>\n",
       "      <td>jlluezfnyr.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>txnmkabufs.mp4</td>\n",
       "      <td>train</td>\n",
       "      <td>dfdc_train_part_2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9460</th>\n",
       "      <td>wxvtaveqvb.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>gpsxfxrjrr.mp4</td>\n",
       "      <td>train</td>\n",
       "      <td>dfdc_train_part_2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3893</th>\n",
       "      <td>rgkauxyqtf.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>gwzeubnydg.mp4</td>\n",
       "      <td>train</td>\n",
       "      <td>dfdc_train_part_14</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               video label        original  split                 dir  processed\n",
       "2532  yakzirnamy.mp4  FAKE  jeyvgixnkm.mp4  train  dfdc_train_part_14      False\n",
       "4407  trnznvybjo.mp4  FAKE  sudzolvppu.mp4  train  dfdc_train_part_48      False\n",
       "4912  gbinyqapyk.mp4  FAKE  ozgcmnllow.mp4  train  dfdc_train_part_48      False\n",
       "6295  yjfdnylpab.mp4  FAKE  ndqxtifvbw.mp4  train  dfdc_train_part_48      False\n",
       "8736  aaujbbfhqu.mp4  FAKE  qiyzfjrloz.mp4  train   dfdc_train_part_2      False\n",
       "4368  hqeldyhmpu.mp4  FAKE  vdqritvjfl.mp4  train  dfdc_train_part_48      False\n",
       "9447  rhncgvckxz.mp4  FAKE  fanibwbmoq.mp4  train   dfdc_train_part_2      False\n",
       "9680  jlluezfnyr.mp4  FAKE  txnmkabufs.mp4  train   dfdc_train_part_2      False\n",
       "9460  wxvtaveqvb.mp4  FAKE  gpsxfxrjrr.mp4  train   dfdc_train_part_2      False\n",
       "3893  rgkauxyqtf.mp4  FAKE  gwzeubnydg.mp4  train  dfdc_train_part_14      False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('processed == False').sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [13:20<00:00,  2.42s/it]\n",
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [13:13<00:00,  3.10s/it]\n",
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [13:07<00:00,  2.84s/it]\n",
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [12:43<00:00,  2.36s/it]\n",
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved batch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a7da4d391f52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mfakes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfakes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_random_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../videos/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mreals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-ba0d85545166>\u001b[0m in \u001b[0;36mextract_random_faces\u001b[0;34m(self, filename, num_faces)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptured_faces\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_faces\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_interations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0miterations\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mv_cap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_cap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch = 0\n",
    "nsample = 300\n",
    "\n",
    "vp = VideoProcessor()\n",
    "\n",
    "while True:\n",
    "    sample = df.query('label==\"FAKE\" and processed == False').sample(nsample)\n",
    "    if len(sample) == 0:\n",
    "        break\n",
    "\n",
    "    fakes = np.empty(shape=(0,d,d,3), dtype=np.int8)\n",
    "    reals = np.empty(shape=(0,d,d,3), dtype=np.int8)\n",
    "\n",
    "    batch += 1\n",
    "    for index, row in tqdm(sample.iterrows(), total=nsample):\n",
    "        f = vp.extract_random_faces('../videos/' + row.dir + '/' + row.video,2)\n",
    "        if len(f) > 0:\n",
    "            fakes = np.append(fakes,f,axis=0)\n",
    "        r = vp.extract_random_faces('../videos/' + row.dir + '/' + row.original,2)\n",
    "        if len(r) > 0:\n",
    "            reals = np.append(reals,r,axis=0)\n",
    "\n",
    "    np.savez(f'../data/train_Xception_{batch}', fakes=fakes, reals=reals)\n",
    "    df.loc[sample.index,'processed'] = f'train_{batch}'\n",
    "    df.to_csv('../data/metadata.csv',index = False)\n",
    "    print(f'saved batch: {batch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
