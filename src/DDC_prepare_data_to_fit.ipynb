{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../videos/dfdc_train_part_48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDetector:\n",
    "    def __init__(self, \n",
    "                 modelFile=\"res10_300x300_ssd_iter_140000.caffemodel\",\n",
    "                 configFile = \"deploy.prototxt\",\n",
    "                 max_interations = 300,\n",
    "                 conf_threshold = 0.60,\n",
    "                 normalized_dim = (32,32)):\n",
    "        self.modelFile = modelFile\n",
    "        self.configFile = configFile\n",
    "        self.max_interations = max_interations\n",
    "        self.conf_threshold = conf_threshold\n",
    "        self.normalized_dim = normalized_dim\n",
    "        self.net = cv2.dnn.readNetFromCaffe(self.configFile, self.modelFile)\n",
    "        \n",
    "    def extract_random_faces(self, filename, num_faces):\n",
    "        captured_faces = []\n",
    "        iterations = 0\n",
    "        v_cap = cv2.VideoCapture(filename)\n",
    "        v_length = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        while len(captured_faces) < num_faces and iterations < self.max_interations:\n",
    "            iterations += 1\n",
    "            v_cap.set(1, np.random.randint(v_length)-1)\n",
    "            \n",
    "            ret, img = v_cap.read()\n",
    "        \n",
    "            if ret == True:\n",
    "                (h, w) = img.shape[:2]\n",
    "                blob = cv2.dnn.blobFromImage(cv2.resize(img, (300, 300)), 1.0, (300, 300), (103.93, 116.77, 123.68))\n",
    "                self.net.setInput(blob)\n",
    "                detections = self.net.forward()\n",
    "                for i in range(detections.shape[2]):\n",
    "                    confidence = detections[0, 0, i, 2]            \n",
    "                    if confidence > self.conf_threshold:\n",
    "                        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                        (x1, y1, x2, y2) = box.astype(\"int\")\n",
    "                        face=img[y1:y2, x1:x2]\n",
    "                        \n",
    "                        # normlize\n",
    "                        face = cv2.resize(face, self.normalized_dim)\n",
    "                        captured_faces.append(face)\n",
    "        \n",
    "        # When everything done, release the video capture and video write objects\n",
    "        v_cap.release()\n",
    "    \n",
    "        return captured_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdc_train_part_48 = '../videos/dfdc_train_part_48'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(dfdc_train_part_48 + '/metadata.json')\n",
    "df = df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>original</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nlrmgdqfnr.mp4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rtdogbpems.mp4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kmkvxunbop.mp4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sxmqvznwwq.mp4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wetheuhcha.mp4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kfzuekxbbb.mp4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bfqlqydtam.mp4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yvfoaoiclp.mp4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qhwkphcmhx.mp4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yuqdwjizdb.mp4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               label original  split\n",
       "nlrmgdqfnr.mp4  REAL      NaN  train\n",
       "rtdogbpems.mp4  REAL      NaN  train\n",
       "kmkvxunbop.mp4  REAL      NaN  train\n",
       "sxmqvznwwq.mp4  REAL      NaN  train\n",
       "wetheuhcha.mp4  REAL      NaN  train\n",
       "kfzuekxbbb.mp4  REAL      NaN  train\n",
       "bfqlqydtam.mp4  REAL      NaN  train\n",
       "yvfoaoiclp.mp4  REAL      NaN  train\n",
       "qhwkphcmhx.mp4  REAL      NaN  train\n",
       "yuqdwjizdb.mp4  REAL      NaN  train"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query(\"label == 'REAL'\").sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd=FaceDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:05<00:00,  2.07s/it]\n"
     ]
    }
   ],
   "source": [
    "nr = 3\n",
    "nf = 4\n",
    "faces = np.empty(shape=(0,32,32,3), dtype=np.int8)\n",
    "labels = np.empty(shape=(0, 1), dtype=\"<U5\")\n",
    "for index, row in tqdm(df.sample(nr).iterrows(), total=nr):\n",
    "    images = fd.extract_random_faces(dfdc_train_part_48+'/'+index, nf)\n",
    "    if len(images) > 0:\n",
    "        faces = np.append(faces,images,axis=0)\n",
    "        labels = np.append(labels, np.full(shape=(len(images),1), fill_value=row.label, dtype=\"<U5\"),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(faces) == len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('train',faces=faces, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['FAKE'],\n",
       "       ['FAKE'],\n",
       "       ['FAKE'],\n",
       "       ['FAKE'],\n",
       "       ['FAKE'],\n",
       "       ['FAKE'],\n",
       "       ['FAKE'],\n",
       "       ['FAKE'],\n",
       "       ['FAKE'],\n",
       "       ['FAKE'],\n",
       "       ['FAKE'],\n",
       "       ['FAKE']], dtype='<U5')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
